{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "# 3_DeepLearning_Model_Training.ipynb\n",
        "\n",
        "Ce notebook est dédié à la construction, à l'entraînement et à l'évaluation\n",
        "d'un modèle de recommandation basé sur les réseaux neuronaux profonds (Deep Learning).\n",
        "Nous utiliserons TensorFlow/Keras pour implémenter une architecture de type\n",
        "Neural Collaborative Filtering (NCF) simplifiée.\n",
        "\"\"\"\n",
        "\n",
        "# 1. Importation des bibliothèques nécessaires\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Bibliothèques importées avec succès.\")\n",
        "print(f\"Version de TensorFlow : {tf.__version__}\")\n",
        "\n",
        "# Assurer la reproductibilité\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 2. Chargement des données prétraitées et des mappings\n",
        "try:\n",
        "    train_df = pd.read_csv('../data/train_ratings.csv')\n",
        "    val_df = pd.read_csv('../data/val_ratings.csv')\n",
        "    test_df = pd.read_csv('../data/test_ratings.csv')\n",
        "\n",
        "    user_to_id = np.load('../data/user_to_id.npy', allow_pickle=True).item()\n",
        "    id_to_user = np.load('../data/id_to_user.npy', allow_pickle=True).item()\n",
        "    movie_to_id = np.load('../data/movie_to_id.npy', allow_pickle=True).item()\n",
        "    id_to_movie = np.load('../data/id_to_movie.npy', allow_pickle=True).item()\n",
        "\n",
        "    print(\"Données d'entraînement, de validation, de test et mappings chargés avec succès.\")\n",
        "    print(f\"Train DataFrame shape: {train_df.shape}\")\n",
        "    print(f\"Validation DataFrame shape: {val_df.shape}\")\n",
        "    print(f\"Test DataFrame shape: {test_df.shape}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur : Les fichiers de données ou de mappings n'ont pas été trouvés.\")\n",
        "    print(\"Assure-toi d'avoir exécuté le notebook '1_EDA_Preprocessing.ipynb' au préalable.\")\n",
        "    exit()\n",
        "\n",
        "# Déterminer le nombre total d'utilisateurs et de films mappés\n",
        "n_users_mapped = len(user_to_id)\n",
        "n_movies_mapped = len(movie_to_id)\n",
        "print(f\"Nombre d'utilisateurs mappés : {n_users_mapped}\")\n",
        "print(f\"Nombre de films mappés : {n_movies_mapped}\")\n",
        "\n",
        "# 3. Préparation des données pour TensorFlow/Keras\n",
        "# Les modèles Keras attendent des tableaux NumPy comme entrées.\n",
        "X_train_users = train_df['user_id_mapped'].values\n",
        "X_train_movies = train_df['movie_id_mapped'].values\n",
        "y_train = train_df['rating'].values\n",
        "\n",
        "X_val_users = val_df['user_id_mapped'].values\n",
        "X_val_movies = val_df['movie_id_mapped'].values\n",
        "y_val = val_df['rating'].values\n",
        "\n",
        "X_test_users = test_df['user_id_mapped'].values\n",
        "X_test_movies = test_df['movie_id_mapped'].values\n",
        "y_test = test_df['rating'].values\n",
        "\n",
        "print(\"\\nDonnées préparées pour l'entraînement du modèle Deep Learning.\")\n",
        "print(f\"Exemple de données d'entraînement : user_id={X_train_users[0]}, movie_id={X_train_movies[0]}, rating={y_train[0]}\")\n",
        "\n",
        "# 4. Construction du modèle de Deep Learning (NCF simplifié)\n",
        "# L'idée est d'apprendre des embeddings (représentations vectorielles denses) pour chaque utilisateur et chaque film,\n",
        "# puis de les combiner et de les passer à travers un réseau de neurones pour prédire la note.\n",
        "\n",
        "# Dimensions des embeddings\n",
        "embedding_dim = 50 # Une taille courante pour les embeddings\n",
        "\n",
        "# Entrée pour les IDs d'utilisateurs\n",
        "user_input = Input(shape=(1,), name='user_input')\n",
        "# Couche d'embedding pour les utilisateurs\n",
        "# input_dim = nombre total d'utilisateurs uniques + 1 (pour gérer les IDs de 0 à N-1)\n",
        "user_embedding = Embedding(input_dim=n_users_mapped + 1, output_dim=embedding_dim, name='user_embedding')(user_input)\n",
        "user_vec = Flatten(name='user_flatten')(user_embedding) # Aplatir l'embedding\n",
        "\n",
        "# Entrée pour les IDs de films\n",
        "movie_input = Input(shape=(1,), name='movie_input')\n",
        "# Couche d'embedding pour les films\n",
        "movie_embedding = Embedding(input_dim=n_movies_mapped + 1, output_dim=embedding_dim, name='movie_embedding')(movie_input)\n",
        "movie_vec = Flatten(name='movie_flatten')(movie_embedding) # Aplatir l'embedding\n",
        "\n",
        "# Concaténer les vecteurs d'embeddings utilisateur et film\n",
        "concat = Concatenate(name='concatenate_embeddings')([user_vec, movie_vec])\n",
        "\n",
        "# Couches denses (MLP)\n",
        "# Ces couches apprennent des interactions non linéaires entre les embeddings\n",
        "dense_1 = Dense(128, activation='relu', name='dense_1')(concat)\n",
        "dense_2 = Dense(64, activation='relu', name='dense_2')(dense_1)\n",
        "dense_3 = Dense(32, activation='relu', name='dense_3')(dense_2)\n",
        "\n",
        "# Couche de sortie\n",
        "# Une seule neurone pour prédire la note. Pas d'activation pour une régression directe,\n",
        "# ou sigmoid si on veut compresser la sortie entre 0 et 1 puis scaler.\n",
        "# Ici, nous allons prédire directement la note.\n",
        "output = Dense(1, activation=None, name='output_rating')(dense_3)\n",
        "\n",
        "# Créer le modèle\n",
        "model = Model(inputs=[user_input, movie_input], outputs=output)\n",
        "\n",
        "# Compiler le modèle\n",
        "# Utilisation de l'optimiseur Adam et de l'erreur quadratique moyenne (MSE) comme fonction de perte.\n",
        "# Nous suivrons aussi le MAE.\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "print(\"\\n--- Architecture du modèle de Deep Learning ---\")\n",
        "model.summary()\n",
        "\n",
        "# 5. Entraînement du modèle\n",
        "# Utilisation de Callbacks pour améliorer l'entraînement :\n",
        "# - EarlyStopping: Arrête l'entraînement si la performance sur l'ensemble de validation ne s'améliore plus.\n",
        "# - ModelCheckpoint: Sauvegarde le meilleur modèle (basé sur la performance de validation).\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', # Surveiller la perte sur l'ensemble de validation\n",
        "    patience=5,          # Attendre 5 époques sans amélioration avant d'arrêter\n",
        "    restore_best_weights=True # Restaurer les poids du meilleur modèle trouvé\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='../data/deeprec_model.h5', # Chemin pour sauvegarder le modèle\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True, # Sauvegarder uniquement le meilleur modèle\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n--- Début de l'entraînement du modèle ---\")\n",
        "history = model.fit(\n",
        "    [X_train_users, X_train_movies], # Entrées pour les utilisateurs et les films\n",
        "    y_train,                         # Notes réelles\n",
        "    epochs=20,                       # Nombre maximal d'époques\n",
        "    batch_size=256,                  # Taille du batch\n",
        "    validation_data=([X_val_users, X_val_movies], y_val), # Données de validation\n",
        "    callbacks=[early_stopping, model_checkpoint], # Utilisation des callbacks\n",
        "    verbose=1\n",
        ")\n",
        "print(\"\\n--- Entraînement du modèle terminé ! ---\")\n",
        "\n",
        "# 6. Visualisation de l'historique d'entraînement\n",
        "print(\"\\n--- Visualisation de l'historique d'entraînement ---\")\n",
        "\n",
        "# Plot de la perte (loss)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Perte d\\'entraînement')\n",
        "plt.plot(history.history['val_loss'], label='Perte de validation')\n",
        "plt.title('Perte du modèle pendant l\\'entraînement')\n",
        "plt.xlabel('Époque')\n",
        "plt.ylabel('Perte (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot de la MAE\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='MAE d\\'entraînement')\n",
        "plt.plot(history.history['val_mae'], label='MAE de validation')\n",
        "plt.title('MAE du modèle pendant l\\'entraînement')\n",
        "plt.xlabel('Époque')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 7. Évaluation du modèle sur l'ensemble de test\n",
        "print(\"\\n--- Évaluation du modèle sur l'ensemble de test ---\")\n",
        "\n",
        "# Charger le meilleur modèle sauvegardé par ModelCheckpoint\n",
        "best_model = tf.keras.models.load_model('../data/deeprec_model.h5')\n",
        "\n",
        "# Faire des prédictions sur l'ensemble de test\n",
        "predictions_dl = best_model.predict([X_test_users, X_test_movies])\n",
        "\n",
        "# Les prédictions sont des tableaux NumPy, aplatissons-les si nécessaire\n",
        "predictions_dl = predictions_dl.flatten()\n",
        "\n",
        "# S'assurer que les prédictions sont dans la plage des notes (1 à 5)\n",
        "predictions_dl = np.clip(predictions_dl, 1.0, 5.0)\n",
        "\n",
        "# Calcul des métriques RMSE et MAE\n",
        "rmse_dl = sqrt(mean_squared_error(y_test, predictions_dl))\n",
        "mae_dl = mean_absolute_error(y_test, predictions_dl)\n",
        "\n",
        "print(f\"RMSE (Deep Learning Model) sur l'ensemble de test : {rmse_dl:.4f}\")\n",
        "print(f\"MAE (Deep Learning Model) sur l'ensemble de test : {mae_dl:.4f}\")\n",
        "\n",
        "# Sauvegarder les prédictions pour une analyse future si nécessaire\n",
        "pd.DataFrame({\n",
        "    'user_id_mapped': X_test_users,\n",
        "    'movie_id_mapped': X_test_movies,\n",
        "    'true_rating': y_test,\n",
        "    'predicted_rating_dl': predictions_dl\n",
        "}).to_csv('../data/dl_predictions.csv', index=False)\n",
        "print(\"\\nPrédictions du modèle Deep Learning sauvegardées sous '../data/dl_predictions.csv'.\")\n",
        "\n",
        "print(\"\\n--- Entraînement et évaluation du modèle Deep Learning terminés ! ---\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "dz6YVt6tUiFN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}